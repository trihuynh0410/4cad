{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "import numpy as np\n",
    "\n",
    "def DKPCA(features_scaled, n_components=None, kernel=None):\n",
    "    \n",
    "    # Perform KPCA\n",
    "    kpca = KernelPCA(n_components=n_components, kernel=kernel, fit_inverse_transform=True)\n",
    "    kpca.fit(features_scaled)\n",
    "    \n",
    "    # Transform the features\n",
    "    projections = kpca.transform(features_scaled)\n",
    "    \n",
    "    # Determine thresholds\n",
    "    thresholds = {}\n",
    "    for j in range(projections.shape[1]):\n",
    "        s = skew(projections[:, j])\n",
    "        k = kurtosis(projections[:, j], fisher=False)\n",
    "        stat, p_value = shapiro(projections[:, j])\n",
    "        if p_value > 0.05:\n",
    "            mean = np.mean(projections[:, j])\n",
    "            std = np.std(projections[:, j])\n",
    "            thresholds[j] = mean + 2 * std  # 95% confidence interval\n",
    "        else:\n",
    "            thresholds[j] = np.percentile(projections[:, j], 95)  # 95th percentile\n",
    "    \n",
    "    # Select subset indices\n",
    "    subset_indices = []\n",
    "    for j in range(projections.shape[1]):\n",
    "        candidate_indices = np.where(projections[:, j] < thresholds[j])[0]\n",
    "        if candidate_indices.size > 0:\n",
    "            subset_index = candidate_indices[np.argmax(projections[candidate_indices, j])]\n",
    "            subset_indices.append(subset_index)\n",
    "    \n",
    "    subset_indices = list(set(subset_indices))\n",
    "    \n",
    "    # Compute the new kernel matrix using the same kernel function\n",
    "    K_new = pairwise_kernels(features_scaled[subset_indices, :], features_scaled, metric=kernel)\n",
    "    \n",
    "    # Compute the DKPCA features\n",
    "    eigenvectors_subset = kpca.eigenvectors_[subset_indices, :]\n",
    "    features_dkpca = np.dot(K_new.T, eigenvectors_subset)\n",
    "    \n",
    "    return features_dkpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "# from DKPCA import DKPCA  # Assuming DKPCA is a custom function or module you have\n",
    "\n",
    "def parallel(features_scaled, n_component, feature_reduction_method, kernel=None, labels= None):\n",
    "    if feature_reduction_method == 'pca':\n",
    "        pca = PCA(n_components=n_component)\n",
    "        features_reduced = pca.fit_transform(features_scaled)\n",
    "    elif feature_reduction_method == 'kpca':\n",
    "        kpca = KernelPCA(n_components=n_component, kernel=kernel)\n",
    "        features_reduced = kpca.fit_transform(features_scaled)\n",
    "    elif feature_reduction_method == 'dkpca':\n",
    "        features_reduced = DKPCA(features_scaled, n_components=n_component, kernel=kernel)\n",
    "    else:\n",
    "        features_reduced = features_scaled\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.3, random_state=42)\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    dtrain = DMatrix(X_train, label=y_train)\n",
    "    dtest = DMatrix(X_test, label=y_test)\n",
    "\n",
    "    classifiers = [xgb.XGBClassifier(device = 'cuda'), KNeighborsClassifier(n_neighbors= 14), LinearDiscriminantAnalysis(solver = 'lsqr')]\n",
    "    clf_names = ['XGBClassifier', 'KNeighborsClassifier', 'LinearDiscriminantAnalysis']\n",
    "    local_results = []\n",
    "    for clf, clf_name in zip(classifiers, clf_names):\n",
    "\n",
    "        if clf_name == 'XGBClassifier':\n",
    "            param = clf.get_xgb_params()\n",
    "            param['objective'] = 'multi:softmax'\n",
    "            param['num_class'] = len(set(y_train))\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "            bst = xgb.train(param, dtrain)\n",
    "            y_pred = bst.predict(dtest)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        local_results.append((report['accuracy'], clf_name, feature_reduction_method, n_component, kernel))\n",
    "\n",
    "    return local_results\n",
    "\n",
    "def evaluate_models(df, feature_reduction=None, components_range=None, kernels=None):\n",
    "    results = []\n",
    "    features = df.iloc[:, 2:]\n",
    "    labels = df.iloc[:, 1]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    if feature_reduction:\n",
    "        loop_range = components_range\n",
    "    else:\n",
    "        loop_range = [None]\n",
    "\n",
    "    for kernel in kernels:\n",
    "        with ThreadPoolExecutor(max_workers=len(loop_range)) as executor:\n",
    "            future_tasks = {executor.submit(parallel, features_scaled, n_component, feature_reduction, kernel, labels): n_component for n_component in loop_range}\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_tasks):\n",
    "                results.extend(future.result())\n",
    "                \n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_5_results = results[:5]\n",
    "    return top_5_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your types and regions\n",
    "types = [\"ori\", \"segment\", \"shear\", \"shearseg\"]\n",
    "regions = [\"hippo\", \"ven\"]\n",
    "\n",
    "file_path = f\"/mnt/data_lab513/tramy/4CAD/data/entropy/Entropy_{regions[0]}_{types[0]}.csv\"\n",
    "merged_df = pd.read_csv(file_path)\n",
    "\n",
    "# Loop over each type and region starting from the second CSV file\n",
    "for r in regions:\n",
    "    for t in types[1:]:\n",
    "        # Construct the file path\n",
    "        file_path = f\"/mnt/data_lab513/tramy/4CAD/data/entropy/Entropy_{r}_{t}.csv\"\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Merge the DataFrame with the merged DataFrame on the common column\n",
    "        merged_df = pd.merge(merged_df, df, on=[\"subject\", \"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'hippo_ori_Scale3_Window4_SampEn' is not significant.\n",
      "Feature 'hippo_ori_Scale3_Window4_DispEn' is not significant.\n",
      "Feature 'hippo_ori_Scale3_Window4_PermEn' is not significant.\n",
      "Feature 'hippo_ori_Scale3_Window4_EspEn' is not significant.\n",
      "Feature 'hippo_ori_Scale4_Window4_SampEn' is not significant.\n",
      "Feature 'hippo_ori_Scale4_Window4_FuzzEn' is not significant.\n",
      "Feature 'hippo_ori_Scale4_Window4_DispEn' is not significant.\n",
      "Feature 'hippo_ori_Scale4_Window4_PermEn' is not significant.\n",
      "Feature 'hippo_ori_Scale4_Window4_EspEn' is not significant.\n",
      "Feature 'hippo_shear_Scale3_Window4_SampEn' is not significant.\n",
      "Feature 'hippo_shear_Scale3_Window4_FuzzEn' is not significant.\n",
      "Feature 'hippo_shear_Scale3_Window4_DistEn' is not significant.\n",
      "Feature 'hippo_shear_Scale3_Window4_PermEn' is not significant.\n",
      "Feature 'hippo_shear_Scale3_Window4_EspEn' is not significant.\n",
      "Feature 'hippo_shear_Scale4_Window4_SampEn' is not significant.\n",
      "Feature 'hippo_shear_Scale4_Window4_FuzzEn' is not significant.\n",
      "Feature 'hippo_shear_Scale4_Window4_DistEn' is not significant.\n",
      "Feature 'hippo_shear_Scale4_Window4_PermEn' is not significant.\n",
      "Feature 'hippo_shear_Scale4_Window4_EspEn' is not significant.\n",
      "Feature 'hippo_shearseg_Scale3_Window4_SampEn' is not significant.\n",
      "Feature 'hippo_shearseg_Scale4_Window4_PermEn' is not significant.\n",
      "Feature 'ven_shear_Scale3_Window4_SampEn' is not significant.\n",
      "Feature 'ven_shear_Scale3_Window4_FuzzEn' is not significant.\n",
      "Feature 'ven_shear_Scale3_Window4_PermEn' is not significant.\n",
      "Feature 'ven_shear_Scale4_Window4_SampEn' is not significant.\n",
      "Feature 'ven_shear_Scale4_Window4_FuzzEn' is not significant.\n",
      "Feature 'ven_shear_Scale4_Window4_PermEn' is not significant.\n",
      "Feature 'ven_shearseg_Scale4_Window4_PermEn' is not significant.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = merged_df.iloc[:, 2:]\n",
    "cols_to_keep = features.columns[(features != 0).any(axis=0)]\n",
    "features = features[cols_to_keep]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "labels = merged_df.iloc[:, 1]\n",
    "not_significant_features = []\n",
    "\n",
    "for feature in merged_df.columns:\n",
    "    if feature not in ['subject', 'label']:\n",
    "        class0 = merged_df[merged_df['label'] == 0][feature].dropna()\n",
    "        class1 = merged_df[merged_df['label'] == 1][feature].dropna()\n",
    "        class2 = merged_df[merged_df['label'] == 2][feature].dropna()\n",
    "        class3 = merged_df[merged_df['label'] == 3][feature].dropna()\n",
    "\n",
    "        if len(class0) < 2 or len(class1) < 2 or len(class2) < 2 or len(class3) < 2:\n",
    "            continue\n",
    "        F, p = stats.f_oneway(class0, class1, class2, class3)    \n",
    "        if p > 0.05:\n",
    "            not_significant_features.append(feature)\n",
    "\n",
    "for feature in not_significant_features:\n",
    "    print(f\"Feature '{feature}' is not significant.\")\n",
    "\n",
    "merged_df = merged_df.drop(columns=not_significant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df[merged_df['label'].isin([0, 1, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.424390243902439, 'KNeighborsClassifier', None, None, 'rbf')\n",
      "(0.40487804878048783, 'LinearDiscriminantAnalysis', None, None, 'rbf')\n",
      "(0.3804878048780488, 'XGBClassifier', None, None, 'rbf')\n"
     ]
    }
   ],
   "source": [
    "top_5_None = evaluate_models(merged_df, feature_reduction=None, kernels=['rbf'])\n",
    "for result in top_5_None:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'pca', 5, 'linear')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'pca', 4, 'linear')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'pca', 5, 'poly')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'pca', 4, 'poly')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'pca', 5, 'rbf')\n"
     ]
    }
   ],
   "source": [
    "top_5_PCA = evaluate_models(merged_df,feature_reduction='pca', components_range=range(2, 20), kernels = kernels)\n",
    "for result in top_5_PCA:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4975609756097561, 'XGBClassifier', 'kpca', 16, 'poly')\n",
      "(0.4926829268292683, 'LinearDiscriminantAnalysis', 'kpca', 6, 'sigmoid')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'kpca', 4, 'linear')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'kpca', 5, 'linear')\n",
      "(0.47804878048780486, 'LinearDiscriminantAnalysis', 'kpca', 5, 'rbf')\n"
     ]
    }
   ],
   "source": [
    "top_5_KPCA = evaluate_models(merged_df, feature_reduction='kpca', components_range=range(2, 20), kernels = kernels)\n",
    "for result in top_5_KPCA:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48292682926829267, 'LinearDiscriminantAnalysis', 'dkpca', 11, 'rbf')\n",
      "(0.48292682926829267, 'LinearDiscriminantAnalysis', 'dkpca', 4, 'sigmoid')\n",
      "(0.47317073170731705, 'LinearDiscriminantAnalysis', 'dkpca', 4, 'linear')\n",
      "(0.47317073170731705, 'LinearDiscriminantAnalysis', 'dkpca', 6, 'sigmoid')\n",
      "(0.4682926829268293, 'LinearDiscriminantAnalysis', 'dkpca', 5, 'linear')\n"
     ]
    }
   ],
   "source": [
    "top_5_DKPCA = evaluate_models(merged_df, feature_reduction='dkpca', components_range=range(2, 20), kernels = kernels)\n",
    "for result in top_5_DKPCA:\n",
    "     print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
