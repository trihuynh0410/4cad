{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from scipy.stats import skew, kurtosis, shapiro\n",
    "import numpy as np\n",
    "\n",
    "def DKPCA(features_scaled, n_components=None, kernel=None):\n",
    "    \n",
    "    # Perform KPCA\n",
    "    kpca = KernelPCA(n_components=n_components, kernel=kernel, fit_inverse_transform=True)\n",
    "    kpca.fit(features_scaled)\n",
    "    \n",
    "    # Transform the features\n",
    "    projections = kpca.transform(features_scaled)\n",
    "    \n",
    "    # Determine thresholds\n",
    "    thresholds = {}\n",
    "    for j in range(projections.shape[1]):\n",
    "        s = skew(projections[:, j])\n",
    "        k = kurtosis(projections[:, j], fisher=False)\n",
    "        stat, p_value = shapiro(projections[:, j])\n",
    "        if p_value > 0.05:\n",
    "            mean = np.mean(projections[:, j])\n",
    "            std = np.std(projections[:, j])\n",
    "            thresholds[j] = mean + 2 * std  # 95% confidence interval\n",
    "        else:\n",
    "            thresholds[j] = np.percentile(projections[:, j], 95)  # 95th percentile\n",
    "    \n",
    "    # Select subset indices\n",
    "    subset_indices = []\n",
    "    for j in range(projections.shape[1]):\n",
    "        candidate_indices = np.where(projections[:, j] < thresholds[j])[0]\n",
    "        if candidate_indices.size > 0:\n",
    "            subset_index = candidate_indices[np.argmax(projections[candidate_indices, j])]\n",
    "            subset_indices.append(subset_index)\n",
    "    \n",
    "    subset_indices = list(set(subset_indices))\n",
    "    \n",
    "    # Compute the new kernel matrix using the same kernel function\n",
    "    K_new = pairwise_kernels(features_scaled[subset_indices, :], features_scaled, metric=kernel)\n",
    "    \n",
    "    # Compute the DKPCA features\n",
    "    eigenvectors_subset = kpca.eigenvectors_[subset_indices, :]\n",
    "    features_dkpca = np.dot(K_new.T, eigenvectors_subset)\n",
    "    \n",
    "    return features_dkpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "# from DKPCA import DKPCA  # Assuming DKPCA is a custom function or module you have\n",
    "\n",
    "def parallel(features_scaled, scales_subset, windows_subset, n_component, feature_reduction_method, kernel=None, labels= None):\n",
    "    if feature_reduction_method == 'pca':\n",
    "        pca = PCA(n_components=n_component)\n",
    "        features_reduced = pca.fit_transform(features_scaled)\n",
    "    elif feature_reduction_method == 'kpca':\n",
    "        kpca = KernelPCA(n_components=n_component, kernel=kernel)\n",
    "        features_reduced = kpca.fit_transform(features_scaled)\n",
    "    elif feature_reduction_method == 'dkpca':\n",
    "        features_reduced = DKPCA(features_scaled, n_components=n_component, kernel=kernel)\n",
    "    else:\n",
    "        features_reduced = features_scaled\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.3, random_state=42)\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    classifiers = [XGBClassifier(device = 'cuda'), KNeighborsClassifier(n_neighbors= 14), LinearDiscriminantAnalysis(solver = 'lsqr')]\n",
    "    clf_names = ['XGBClassifier', 'KNeighborsClassifier', 'LinearDiscriminantAnalysis']\n",
    "    local_results = []\n",
    "    for clf, clf_name in zip(classifiers, clf_names):\n",
    "        # Train and evaluate the classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Append results\n",
    "        local_results.append((report['accuracy'], scales_subset, windows_subset, clf_name, feature_reduction_method, n_component, kernel))\n",
    "\n",
    "    return local_results\n",
    "\n",
    "def evaluate_models(df, scales, windows, feature_reduction=None, components_range=None, kernels=None):\n",
    "    results = []\n",
    "    features = df.iloc[:, 2:]\n",
    "    labels = df.iloc[:, 1]\n",
    "\n",
    "    for r in range(1, len(scales)+1):\n",
    "        for scales_subset in itertools.combinations(scales, r):\n",
    "            for s in range(1, len(windows)+1):\n",
    "                for windows_subset in itertools.combinations(windows, s):\n",
    "                    features_remove = features\n",
    "                    for scale in scales_subset:\n",
    "                        features_remove = features_remove.loc[:, ~features_remove.columns.str.contains(scale)]\n",
    "                    for window in windows_subset:\n",
    "                        features_remove = features_remove.loc[:, ~features_remove.columns.str.contains(window)]\n",
    "                    if features_remove.empty:\n",
    "                        continue\n",
    "                    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "                    features_scaled = scaler.fit_transform(features_remove)\n",
    "\n",
    "                    if feature_reduction:\n",
    "                        loop_range = components_range\n",
    "                    else:\n",
    "                        loop_range = [None]\n",
    "\n",
    "                    for kernel in kernels:\n",
    "                        with ThreadPoolExecutor(max_workers=len(loop_range)) as executor:\n",
    "                            future_tasks = {executor.submit(parallel, features_scaled, scales_subset, windows_subset, n_component, feature_reduction, kernel, labels): n_component for n_component in loop_range}\n",
    "\n",
    "                            for future in concurrent.futures.as_completed(future_tasks):\n",
    "                                results.extend(future.result())\n",
    "                    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_5_results = results[:5]\n",
    "\n",
    "    return top_5_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/mnt/data_lab513/tramy/EntropyResults_hippo.csv')\n",
    "df2 = pd.read_csv('/mnt/data_lab513/tramy/EntropyResults_ventricles.csv')\n",
    "df3 = pd.read_csv('/mnt/data_lab513/tramy/EntropyResults_ventricles_ori.csv')\n",
    "\n",
    "df = df1.merge(df2, on=['subject', 'label'])\n",
    "df = df.merge(df3, on=['subject', 'label'])\n",
    "\n",
    "\n",
    "\n",
    "#df = df[df['label'].isin([0, 1, 2])]\n",
    "\n",
    "scales = ['Scale2', 'Scale3', 'Scale4']\n",
    "windows = ['Window1', 'Window2', 'Window3', 'Window4']\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'cosine'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by process one: 1.002009630203247 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.51      0.55        63\n",
      "           1       0.48      0.52      0.50        66\n",
      "           2       0.49      0.71      0.58        51\n",
      "           3       0.31      0.21      0.25        52\n",
      "\n",
      "    accuracy                           0.49       232\n",
      "   macro avg       0.47      0.49      0.47       232\n",
      "weighted avg       0.48      0.49      0.48       232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[:, 2:]\n",
    "labels = df.iloc[:, 1]\n",
    "features = features.loc[:, ~features.columns.str.contains(\"Scale2\")]\n",
    "features = features.loc[:, ~features.columns.str.contains(\"Window3\")]\n",
    "features = features.loc[:, ~features.columns.str.contains(\"Window1\")]\n",
    "features = features.loc[:, ~features.columns.str.contains(\"Window2\")]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "kpca = KernelPCA(n_components=14, kernel='cosine')\n",
    "features_reduced = kpca.fit_transform(features_scaled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_reduced, labels, test_size=0.3, random_state=42)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "import time\n",
    "start = time.time()\n",
    "clf = XGBClassifier(device = 'cuda')\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "process_one_time = end - start\n",
    "\n",
    "print(f\"Time taken by process one: {process_one_time} seconds\")\n",
    "y_pred = clf.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = df.iloc[:, 2:]\n",
    "cols_to_keep = features.columns[(features != 0).any(axis=0)]\n",
    "features = features[cols_to_keep]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "labels = df.iloc[:, 1]\n",
    "not_significant_features = []\n",
    "\n",
    "for feature in df.columns:\n",
    "    if feature not in ['subject', 'label']:\n",
    "        class0 = df[df['label'] == 0][feature].dropna()\n",
    "        class1 = df[df['label'] == 1][feature].dropna()\n",
    "        class2 = df[df['label'] == 2][feature].dropna()\n",
    "        class3 = df[df['label'] == 3][feature].dropna()\n",
    "\n",
    "        if len(class0) < 2 or len(class1) < 2 or len(class2) < 2 or len(class3) < 2:\n",
    "            continue\n",
    "        F, p = stats.f_oneway(class0, class1, class2, class3)    \n",
    "        if p > 0.05:\n",
    "            not_significant_features.append(feature)\n",
    "\n",
    "for feature in not_significant_features:\n",
    "    print(f\"Feature '{feature}' is not significant.\")\n",
    "\n",
    "df = df.drop(columns=not_significant_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_None = evaluate_models(df, scales, windows, feature_reduction=None, kernels=['rbf'])\n",
    "for result in top_5_None:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_PCA = evaluate_models(df, scales, windows, feature_reduction='pca', components_range=range(5, 18), kernels = kernels)\n",
    "for result in top_5_PCA:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_KPCA = evaluate_models(df, scales, windows, feature_reduction='kpca', components_range=range(5, 15), kernels = kernels)\n",
    "for result in top_5_KPCA:\n",
    "     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_DKPCA = evaluate_models(df, scales, windows, feature_reduction='dkpca', components_range=range(5, 15), kernels = kernels)\n",
    "for result in top_5_DKPCA:\n",
    "     print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
